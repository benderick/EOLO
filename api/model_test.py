#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YOLO æ¨¡å‹å¿«é€ŸéªŒè¯å·¥å…·
åœ¨ä¸è¿›è¡Œå®Œæ•´è®­ç»ƒçš„æƒ…å†µä¸‹éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®
"""

import warnings
warnings.filterwarnings('ignore')

import torch
import time
import traceback
import sys
from pathlib import Path
from typing import Optional, Any
from ultralytics import YOLO

class YOLOModelValidator:
    """YOLO æ¨¡å‹éªŒè¯å™¨"""
    
    def __init__(self, model_path: str, imgsz: list = [640, 640], device: str = 'cpu'):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            model_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
            device: è¿è¡Œè®¾å¤‡
        """
        self.model_path = model_path
        self.imgsz = imgsz
        self.device = device
        self.model: Optional[YOLO] = None
        self.validation_results = {
            'model_path': model_path,
            'tests': {},
            'overall_status': 'UNKNOWN',
            'errors': [],
            'warnings': []
        }
        
        # å°è¯•åŠ è½½æ¨¡å‹
        self.load_model()
    
    def load_model(self):
        """åŠ è½½ YOLO æ¨¡å‹"""
        try:
            print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {self.model_path}")
            self.model = YOLO(self.model_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(error_msg)
            self.validation_results['errors'].append(error_msg)
            traceback.print_exc()
    
    def log_test_result(self, test_name: str, status: str, message: str, **kwargs):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            'status': status,
            'message': message,
            'timestamp': time.time(),
            **kwargs
        }
        self.validation_results['tests'][test_name] = result
        
        # æ‰“å°ç»“æœ
        status_emoji = {
            'PASS': 'âœ…',
            'FAIL': 'âŒ', 
            'WARNING': 'âš ï¸',
            'SKIP': 'â­ï¸'
        }
        print(f"{status_emoji.get(status, 'â“')} {test_name}: {message}")
    
    def test_model_info(self):
        """æµ‹è¯• 1: æ¨¡å‹ä¿¡æ¯æ£€æŸ¥"""
        print("\nğŸ“Š 1. æ¨¡å‹ä¿¡æ¯æ£€æŸ¥...")
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise Exception("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            info_output = self.model.info(detailed=False, verbose=False)
            self.log_test_result('model_info', 'PASS', "æ¨¡å‹ä¿¡æ¯è·å–æˆåŠŸ")
            return True
            
        except Exception as e:
            self.log_test_result(
                'model_info', 'FAIL',
                f"æ¨¡å‹ä¿¡æ¯è·å–å¤±è´¥: {str(e)}"
            )
            return False
    
    def test_model_parameters(self):
        """æµ‹è¯• 2: æ¨¡å‹å‚æ•°æ£€æŸ¥"""
        print("\nğŸ” 2. æ¨¡å‹å‚æ•°æ£€æŸ¥...")
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise Exception("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            if hasattr(self.model, 'model') and self.model.model is None:
                raise Exception("æ¨¡å‹ç»“æ„æœªæ­£ç¡®åˆå§‹åŒ–")
            
            # è·å–å‚æ•°ç»Ÿè®¡
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'parameters'):
                param_count = sum(p.numel() for p in self.model.model.parameters())
                trainable_count = sum(p.numel() for p in self.model.model.parameters() if p.requires_grad)
                
                self.log_test_result(
                    'parameters', 'PASS',
                    f"æ€»å‚æ•°: {param_count:,}, å¯è®­ç»ƒ: {trainable_count:,}",
                    total_params=param_count,
                    trainable_params=trainable_count
                )
            else:
                self.log_test_result(
                    'parameters', 'SKIP',
                    "æ— æ³•è®¿é—®æ¨¡å‹å‚æ•°ä¿¡æ¯"
                )
            
            return True
            
        except Exception as e:
            self.log_test_result(
                'parameters', 'FAIL',
                f"å‚æ•°æ£€æŸ¥å¤±è´¥: {str(e)}"
            )
            return False
    
    def test_device_compatibility(self):
        """æµ‹è¯• 3: è®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥"""
        print("\nğŸ–¥ï¸ 3. è®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥...")
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise Exception("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
            if self.device == 'cuda':
                if not torch.cuda.is_available():
                    self.log_test_result(
                        'device_compatibility', 'WARNING',
                        "CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU"
                    )
                    self.device = 'cpu'
                    return True
                else:
                    # ç§»åŠ¨æ¨¡å‹åˆ° GPU
                    if hasattr(self.model, 'model') and hasattr(self.model.model, 'cuda'):
                        self.model.model = self.model.model.cuda()
            
            self.log_test_result(
                'device_compatibility', 'PASS',
                f"è®¾å¤‡ {self.device} å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡"
            )
            return True
            
        except Exception as e:
            self.log_test_result(
                'device_compatibility', 'FAIL',
                f"è®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {str(e)}"
            )
            return False
    
    def test_forward_pass(self):
        """æµ‹è¯• 4: å‰å‘ä¼ æ’­"""
        print("\nğŸ”® 4. å‰å‘ä¼ æ’­æµ‹è¯•...")
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise Exception("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥ (ä½¿ç”¨æ­£ç¡®çš„æ•°æ®èŒƒå›´ [0,1])
            test_input = torch.rand(2, 3, self.imgsz[0], self.imgsz[1])
            
            if self.device == 'cuda' and torch.cuda.is_available():
                test_input = test_input.cuda()
            elif self.device == 'cpu':
                test_input = test_input.cpu()
            
            results = {}
            
            # æµ‹è¯•1: ä½¿ç”¨åº•å±‚ PyTorch æ¨¡å‹è¿›è¡Œæ¨ç†
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'eval'):
                self.model.model.eval()
                with torch.no_grad():
                    if callable(self.model.model):
                        outputs_raw = self.model.model(test_input)
                        results['raw_model'] = f"åŸå§‹æ¨¡å‹è¾“å‡ºç±»å‹: {type(outputs_raw)}"
                        if isinstance(outputs_raw, (list, tuple)):
                            results['raw_model'] += f", è¾“å‡ºæ•°é‡: {len(outputs_raw)}"
                            if len(outputs_raw) > 0 and hasattr(outputs_raw[0], 'shape'):
                                results['raw_model'] += f", ç¬¬ä¸€ä¸ªè¾“å‡ºå½¢çŠ¶: {outputs_raw[0].shape}"
            
            # æµ‹è¯•2: ä½¿ç”¨ YOLO é¢„æµ‹æ¥å£
            try:
                outputs_predict = self.model.predict(test_input, verbose=False)
                results['predict_api'] = f"é¢„æµ‹APIè¾“å‡ºç±»å‹: {type(outputs_predict)}"
                if isinstance(outputs_predict, (list, tuple)) and len(outputs_predict) > 0:
                    results['predict_api'] += f", ç»“æœæ•°é‡: {len(outputs_predict)}"
            except Exception as e:
                results['predict_api'] = f"é¢„æµ‹APIå¤±è´¥: {str(e)}"
            
            # è®°å½•æµ‹è¯•ç»“æœ
            if results:
                result_summary = " | ".join(results.values())
                self.log_test_result(
                    'forward_pass', 'PASS',
                    f"å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡ - {result_summary}"
                )
                return True
            else:
                raise Exception("æ‰€æœ‰å‰å‘ä¼ æ’­æ–¹å¼éƒ½å¤±è´¥")
                
        except Exception as e:
            self.log_test_result(
                'forward_pass', 'FAIL',
                f"å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}"
            )
            return False
    
    def test_model_fuse(self):
        """æµ‹è¯• 6: æ¨¡å‹èåˆ"""
        print("\nğŸ”— 6. æ¨¡å‹èåˆæµ‹è¯•...")
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise Exception("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            if hasattr(self.model, 'fuse'):
                self.model.fuse()
                self.log_test_result('model_fuse', 'PASS', "æ¨¡å‹èåˆæˆåŠŸ")
            else:
                self.log_test_result('model_fuse', 'SKIP', "æ¨¡å‹ä¸æ”¯æŒèåˆæ“ä½œ")
            return True
            
        except Exception as e:
            self.log_test_result(
                'model_fuse', 'WARNING',
                f"æ¨¡å‹èåˆå¤±è´¥: {str(e)}"
            )
            return False
    
    def test_gradient_computation(self):
        """æµ‹è¯• 5: æ¢¯åº¦è®¡ç®—ï¼ˆåå‘ä¼ æ’­ï¼‰"""
        print("\nğŸ“ˆ 5. æ¢¯åº¦è®¡ç®—æµ‹è¯•...")
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
            if self.model is None:
                raise Exception("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è®¿é—®åº•å±‚æ¨¡å‹
            if not (hasattr(self.model, 'model') and hasattr(self.model.model, 'parameters')):
                self.log_test_result(
                    'gradient_computation', 'SKIP',
                    "æ— æ³•è®¿é—®æ¨¡å‹å‚æ•°ï¼Œè·³è¿‡æ¢¯åº¦æµ‹è¯•"
                )
                return True
            
            # ä¿å­˜åŸå§‹çŠ¶æ€
            original_training_mode = self.model.model.training
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if self.device == 'cuda' and torch.cuda.is_available():
                self.model.model = self.model.model.cuda()
            elif self.device == 'cpu':
                self.model.model = self.model.model.cpu()
            
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½éœ€è¦æ¢¯åº¦
            for param in self.model.model.parameters():
                param.requires_grad_(True)
            
            # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            self.model.model.train()
            
            # æ¸…é›¶æ‰€æœ‰æ¢¯åº¦
            for param in self.model.model.parameters():
                if param.grad is not None:
                    param.grad.zero_()
            
            # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„è¾“å…¥ï¼Œç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            test_input = torch.randn(2, 3, self.imgsz[0], self.imgsz[1], requires_grad=True)
            
            if self.device == 'cuda' and torch.cuda.is_available():
                test_input = test_input.cuda()
            else:
                test_input = test_input.cpu()
            
            # å‰å‘ä¼ æ’­
            outputs = self.model.model(test_input)
            
            # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
            try:
                # æ ¹æ®è¯Šæ–­ç»“æœï¼ŒYOLOæ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¾“å‡ºæ˜¯list
                if isinstance(outputs, (list, tuple)) and len(outputs) > 0:
                    # é€‰æ‹©ç¬¬ä¸€ä¸ªè¾“å‡ºè®¡ç®—æŸå¤±
                    loss = outputs[0].sum()
                elif hasattr(outputs, 'sum'):
                    loss = outputs.sum()
                else:
                    raise Exception(f"ä¸æ”¯æŒçš„è¾“å‡ºç±»å‹: {type(outputs)}")
                
                # ç¡®ä¿æŸå¤±éœ€è¦æ¢¯åº¦
                if not loss.requires_grad:
                    self.log_test_result(
                        'gradient_computation', 'WARNING',
                        "è®¡ç®—å‡ºçš„æŸå¤±ä¸éœ€è¦æ¢¯åº¦"
                    )
                    return False
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ£€æŸ¥æ¢¯åº¦
                param_with_grad = 0
                total_params = 0
                grad_norms = []
                significant_grads = 0  # æœ‰æ„ä¹‰çš„æ¢¯åº¦æ•°é‡
                
                for param in self.model.model.parameters():
                    if param.requires_grad:
                        total_params += 1
                        if param.grad is not None:
                            param_with_grad += 1
                            grad_norm = param.grad.norm().item()
                            if grad_norm > 1e-8:  # åªè®¡ç®—æœ‰æ„ä¹‰çš„æ¢¯åº¦
                                significant_grads += 1
                                grad_norms.append(grad_norm)
                
                # æ¢å¤åŸå§‹è®­ç»ƒæ¨¡å¼
                self.model.model.train(original_training_mode)
                
                if significant_grads > 0:  # ä¿®æ”¹åˆ¤æ–­æ¡ä»¶
                    avg_grad_norm = sum(grad_norms) / len(grad_norms) if grad_norms else 0
                    self.log_test_result(
                        'gradient_computation', 'PASS',
                        f"æ¢¯åº¦è®¡ç®—æˆåŠŸ - {significant_grads}/{total_params} ä¸ªå‚æ•°æœ‰æœ‰æ•ˆæ¢¯åº¦, å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f}",
                        params_with_grad=significant_grads,
                        total_trainable_params=total_params,
                        avg_grad_norm=avg_grad_norm
                    )
                    return True
                else:
                    self.log_test_result(
                        'gradient_computation', 'FAIL',
                        f"æ²¡æœ‰å‚æ•°è®¡ç®—å‡ºæœ‰æ•ˆæ¢¯åº¦ - æ€»å‚æ•°:{total_params}, æœ‰æ¢¯åº¦:{param_with_grad}, æœ‰æ•ˆæ¢¯åº¦:{significant_grads}"
                    )
                    return False
                    
            except Exception as loss_error:
                # æ¢å¤åŸå§‹è®­ç»ƒæ¨¡å¼
                self.model.model.train(original_training_mode)
                self.log_test_result(
                    'gradient_computation', 'FAIL',
                    f"æŸå¤±è®¡ç®—æˆ–åå‘ä¼ æ’­å¤±è´¥: {str(loss_error)}"
                )
                return False
                
        except Exception as e:
            self.log_test_result(
                'gradient_computation', 'FAIL',
                f"æ¢¯åº¦è®¡ç®—å¤±è´¥: {str(e)}"
            )
            return False
    
    def test_memory_usage(self):
        """æµ‹è¯• 7: å†…å­˜ä½¿ç”¨æ£€æŸ¥"""
        print("\nğŸ’¾ 7. å†…å­˜ä½¿ç”¨æ£€æŸ¥...")
        try:
            if torch.cuda.is_available() and self.device == 'cuda':
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
                
                # åˆ›å»ºæµ‹è¯•è¾“å…¥
                test_input = torch.randn(2, 3, self.imgsz[0], self.imgsz[1]).cuda()
                
                # ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š
                if hasattr(self.model, 'model') and hasattr(self.model.model, 'parameters'):
                    if not next(self.model.model.parameters()).is_cuda:
                        if hasattr(self.model.model, 'cuda'):
                            self.model.model = self.model.model.cuda()
                
                # è¿›è¡Œå‰å‘ä¼ æ’­
                with torch.no_grad():
                    if hasattr(self.model, 'model') and callable(self.model.model):
                        outputs = self.model.model(test_input)
                    else:
                        outputs = self.model.predict(test_input, verbose=False)
                
                peak_memory = torch.cuda.max_memory_allocated()
                memory_mb = peak_memory / 1024 / 1024
                
                self.log_test_result(
                    'memory_check', 'PASS',
                    f"GPU å†…å­˜ä½¿ç”¨: {memory_mb:.2f} MB",
                    memory_usage_mb=memory_mb
                )
                return True
            else:
                self.log_test_result(
                    'memory_check', 'SKIP',
                    "GPU ä¸å¯ç”¨æˆ–æœªæŒ‡å®šï¼Œè·³è¿‡ GPU å†…å­˜æ£€æŸ¥"
                )
                return True
                
        except Exception as e:
            self.log_test_result(
                'memory_check', 'WARNING',
                f"å†…å­˜æ£€æŸ¥å¤±è´¥: {str(e)}"
            )
            return False
    
    def run_validation(self):
        """è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ YOLO æ¨¡å‹éªŒè¯...")
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"å›¾åƒå°ºå¯¸: {self.imgsz}")
        print("=" * 60)
        
        # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ‰€æœ‰æµ‹è¯•
        if self.model is None:
            print("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡æ‰€æœ‰æµ‹è¯•")
            self.validation_results['overall_status'] = 'FAIL'
            return self.validation_results
        
        # å®šä¹‰æµ‹è¯•åˆ—è¡¨ï¼ˆå°†æ¢¯åº¦è®¡ç®—æµ‹è¯•æ”¾åœ¨æ¨¡å‹èåˆä¹‹å‰ï¼‰
        tests = [
            self.test_model_info,
            self.test_model_parameters,
            self.test_device_compatibility,
            self.test_forward_pass,
            self.test_gradient_computation,  # åœ¨æ¨¡å‹èåˆä¹‹å‰æµ‹è¯•æ¢¯åº¦
            self.test_model_fuse,           # æ¨¡å‹èåˆå¯èƒ½ä¼šå½±å“æ¢¯åº¦è®¡ç®—
            self.test_memory_usage,
        ]
        
        passed_tests = 0
        failed_tests = 0
        
        # è¿è¡Œæµ‹è¯•
        start_time = time.time()
        
        for test in tests:
            try:
                if test():
                    passed_tests += 1
                else:
                    failed_tests += 1
            except Exception as e:
                print(f"âŒ æµ‹è¯• {test.__name__} å‘ç”Ÿå¼‚å¸¸: {str(e)}")
                failed_tests += 1
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š éªŒè¯ç»“æœæ€»ç»“:")
        print(f"âœ… é€šè¿‡æµ‹è¯•: {passed_tests}")
        print(f"âŒ å¤±è´¥æµ‹è¯•: {failed_tests}")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        # è®¾ç½®æ€»ä½“çŠ¶æ€
        if failed_tests == 0:
            self.validation_results['overall_status'] = 'PASS'
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹éªŒè¯æˆåŠŸ")
        elif passed_tests > failed_tests:
            self.validation_results['overall_status'] = 'PARTIAL'
            print("âš ï¸ éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œæ¨¡å‹å¯èƒ½å­˜åœ¨ä¸€äº›é—®é¢˜")
        else:
            self.validation_results['overall_status'] = 'FAIL'
            print("âŒ å¤šæ•°æµ‹è¯•å¤±è´¥ï¼Œæ¨¡å‹å­˜åœ¨ä¸¥é‡é—®é¢˜")
        
        self.validation_results['summary'] = {
            'passed': passed_tests,
            'failed': failed_tests,
            'total_time': total_time
        }
        
        return self.validation_results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='YOLO æ¨¡å‹éªŒè¯å·¥å…·')
    parser.add_argument('model_path', help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', default='cuda', choices=['cpu', 'cuda'], help='è¿è¡Œè®¾å¤‡')
    parser.add_argument('--imgsz', nargs=2, type=int, default=[640, 640], help='è¾“å…¥å›¾åƒå°ºå¯¸')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.model_path).exists():
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)
    
    # åˆ›å»ºéªŒè¯å™¨å¹¶è¿è¡ŒéªŒè¯
    validator = YOLOModelValidator(
        model_path=args.model_path,
        device=args.device,
        imgsz=args.imgsz
    )
    
    results = validator.run_validation()
    
    # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
    if results['overall_status'] == 'PASS':
        sys.exit(0)
    elif results['overall_status'] == 'PARTIAL':
        sys.exit(1)
    else:
        sys.exit(2)


if __name__ == "__main__":
    main()
